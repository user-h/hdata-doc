



#### 琐碎代码

##### 程序中主动退出爬虫

```python
# scrapy框架中手动退出爬虫程序
self.crawler.engine.close_spider(self, '验证码尝试三次不正确，退出爬虫')
```

##### 日期格式化

```python
# pip install python-dateutil
import datetime
from dateutil import parser

time_string = "Fri Jan 05 15:41:17 GMT+08:00 2024"
dt_object = parser.parse(time_string)
print(dt_object.strftime("%Y-%m-%d %H:%M:%S"))
# 转换为 UTC 时间
print(dt_object.astimezone(tz=datetime.timezone.utc))
# 或者如果你想转换到本地时区（假设你的系统配置了正确的本地时区）
print(dt_object.astimezone())
```

##### 发送请求并获取请求头中的Cookie

```python
response = requests.get(url, headers=headers)
self.cookies = requests.utils.dict_from_cookiejar(response.cookies)
```

##### json 转字符串乱码解决

```python
# 确保输出的结果中不包含非ASCII字符
json.dumps(response["result"], ensure_ascii=False)
```

##### 使用 lxml 解析数据

```python
data="""
<datastore>
<recordset>
<record><![CDATA[
<div class="fyxx_lsbox">	<div class="btqz"></div>	<div class="fyxx_lsbox_r">		<a title="528国道龙游十里铺至上圩头段改建工程竣（交）工质量评定检测竣（交）工质量评定检测[E3300000007000941003001]招标公告" target="_blank" href="/art/2024/1/12/art_1229682709_457018.html">528国道龙游十里铺至上圩头段改建工程竣（交）工质量评定检测竣（交）工质量评定检测[E3300000007000941003001]招标公告</a> <span>2024-01-12</span> </div></div>]]></record>
</recordset>
</datastore>

# 方法一：工具直接解析（上述数据不太符合，但写法应如此）
from xml.etree import ElementTree as ET
root = ET.fromstring(data)
for element in root.iter():
   # 简单地访问.text属性，因为在大多数情况下解析器已经处理好CDATA了
   if element.text:
      print(element.text)

# 方法二：字符串混合xpath处理
datastore = etree.HTML(data)
for row in datastore.xpath('//record'):
   record = etree.tostring(row).replace(r'<record>', '').replace(r']]&gt;</record>', '')
   record1 = etree.HTML(record)
   print etree.tostring(record1)
"""
```

##### 代码中执行单个爬虫

```python
from scrapy import cmdline
cmdline.execute("scrapy crawl xxx -o ./data/xxx.csv -s LOG_FILE=./logs/xxx.log".split())
```


#### 附：代码示例

##### 爬虫代码

```python
from xxx.items import gpItems
from scrapy.spiders import CrawlSpider, Rule
from scrapy.http import Request

import logging
import base64
from Crypto.Cipher import AES

class XXXSpider(CrawlSpider):
	name = 'XXX'
	allowed_domains = ['www.xxx.com']

	# key：数据接口地址   value：source type_code 一次爬取的页面数
	categoryCodeDict = {
		'网址1{}': '描述1 ID1 1 3',  # 17页    15-31页
		'网址2{}': '描述2 ID2 1 6',  # 179页
	}
	headers = {
	}

	def start_requests(self):
		for key, value in self.categoryCodeDict.items():
			start_page = int(value.split(' ')[2])
			page = int(value.split(' ')[3])
			for index in range(start_page, page):
				url = key.format(str(index))
				logging.info('爬取总页数：{}  页面链接：{}'.format(page - 1, url))
				yield Request(url=url, headers=self.headers, method='POST', callback=self.parse, meta={'categoryCode': key, 'categoryText': value})

	def parse(self, response):
		categoryText1 = response.meta['categoryText'].split(' ')[0]
		categoryText2 = response.meta['categoryText'].split(' ')[1]

		rows = response.xpath('//ul[@class="article-list-a"]/li')
		for row in rows:
			i = gpItems()
			i['publishTime'] = row.xpath('div[1]/div[@class="list-times"]/text()').extract_first().strip()
			url = row.xpath('div[1]/a/@href').extract_first().strip()
			ccc = url.split('/')[-1].split('.')[0]
			ddd = self.aes_encode(ccc, '公钥编码').replace('/', '^').replace("==", "")
			i['url'] = url.replace(ccc, ddd)
			i['title'] = ''.join(row.xpath('div[1]/a/text()').extract()).strip()
			ad_name = row.xpath('div[1]/a/label/text()').extract_first()
			if ad_name:
				ad_name = ad_name.strip().replace('【', '').replace('】', '')
				i['ad_name'] = 'sss' if ad_name == 'bbb' else ad_name
			else:
				i['ad_name'] = 'xxx'
			i['source'] = categoryText1
			i['type_code'] = categoryText2
			yield Request(i['url'], headers=self.headers, meta={'item': i}, callback=self.parse_item)

	def parse_item(self, response):
		i = response.meta['item']
		i['htmlcontent'] = response.xpath('//div[@class="class1111"]').extract_first().strip()
		return i

	def aes_encode(self, data, key):
		global aes
		while len(data) % 16 != 0:  # 补足字符串长度为16的倍数
			data += (16 - len(data) % 16) * chr(16 - len(data) % 16)
			data = str.encode(data)
			aes = AES.new(str.encode(key), AES.MODE_ECB)  # 初始化加密器
		return str(base64.encodebytes(aes.encrypt(data)), encoding='utf8').replace('\n', '')  # 加密
```

##### 中间件使用动态代理IP测试

```python
class AutotestDownloaderMiddleware(object):
    delay_sec = [0.1, 0.2, 0.3, 0.4, 0.5]   # , 0.6, 0.7, 0.8, 0.9
    proxy_ip_dict = {}
    # 需要走代理的爬虫
    spiders_list = ['XXX']

    def spider_opened(self, spider):
        # 初始化代理池
        if spider.name in self.spiders_list:
            while len(self.proxy_ip_dict) < 6:
                time.sleep(5)
                self.get_proxy()

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        if spider.name in self.spiders_list:
            # time.sleep(random.choice(self.delay_sec))
            proxy_ip = self.determine(int(time.time()*1000))
            print('request.meta[\'proxy\'] = {proxy_ip}'.format(proxy_ip=proxy_ip))
            print('proxy_ip_list：{}'.format(self.proxy_ip_dict))
            request.meta['proxy'] = proxy_ip

    def process_response(self, request, response, spider):
        return response

    def process_exception(self, request, exception, spider):
        pass

    def get_proxy(self, proxy_pop=''):
        curtime = int(time.time()*1000)
        api_url = '购买的动态IP接口'   # 返回值为 IP:PORT,可用时间(毫秒)
        response = requests.get(api_url).text.strip()
        proxy = 'http://' +response.split(',')[0]
        mill = int(response.split(',')[1])
        print('从api获取动态ip：{}'.format(response))
        # self.proxy_ip_list.append((proxy, curtime + mill))
        # 已经有,更新；没有,删除旧的,插入新的   proxy_pop为空字符串时初始化字典
        if proxy_pop == '' or self.proxy_ip_dict.get(proxy) is not None:
            pass
        else:
            print("==================删除ip======{}".format(proxy_pop))
            self.proxy_ip_dict.pop(proxy_pop)
        self.proxy_ip_dict[proxy] = curtime + mill - 2000 # 2s预留时间差
        # return (proxy, curtime + mill)
        return proxy

    def determine(self, curmill):
        # 获取ip
        item1 = random.choice(list(self.proxy_ip_dict.keys()))
        print('可用时间：{}  当前时间：{}'.format(self.proxy_ip_dict[item1], curmill))
        if self.proxy_ip_dict[item1] > curmill:
            # 若可用，返回
            # return tuple1[0]
            return item1
        else:
            print("================== ip：{}  过期时间：{}  当前时间：{} ======".format(item1, self.proxy_ip_dict[item1], int(time.time()*1000) ))
            # 若不可用，从列表移除  重新获取
            return self.get_proxy(item1)
```