---
title: Python爬虫
date: 2024-01-11 09:28:56
permalink: /pages/f799c7/
---




#### 琐碎代码

##### 程序中主动退出爬虫

```python
# scrapy框架中手动退出爬虫程序
self.crawler.engine.close_spider(self, '验证码尝试三次不正确，退出爬虫')
```

##### 日期格式化

```python
# pip install python-dateutil
import datetime
from dateutil import parser

time_string = "Fri Jan 05 15:41:17 GMT+08:00 2024"
dt_object = parser.parse(time_string)
print(dt_object.strftime("%Y-%m-%d %H:%M:%S"))
# 转换为 UTC 时间
print(dt_object.astimezone(tz=datetime.timezone.utc))
# 或者如果你想转换到本地时区（假设你的系统配置了正确的本地时区）
print(dt_object.astimezone())
```

##### 发送请求并获取请求头中的Cookie

```python
response = requests.get(url, headers=headers)
self.cookies = requests.utils.dict_from_cookiejar(response.cookies)
```

##### json 转字符串乱码解决

```python
# 确保输出的结果中不包含非ASCII字符
json.dumps(response["result"], ensure_ascii=False)
```

##### requests 库请求结果显示中文

```python
# 设置编码
response.encoding='utf-8'
print(response.text)
```

##### 使用 lxml 解析数据

```python
data="""
<datastore>
<recordset>
<record><![CDATA[
<div class="fyxx_lsbox">	<div class="btqz"></div>	<div class="fyxx_lsbox_r">		<a title="xxxxxxxxxxxxxxxxxxxxxx" target="_blank" href="/art/2024/1/12/art_1229682709_457018.html">xxxxxxxxxxxxxxxxxxxxxx</a> <span>2024-01-12</span> </div></div>]]></record>
</recordset>
</datastore>
"""

# 方法一：工具直接解析（上述数据不太符合，但写法应如此）
from xml.etree import ElementTree as ET
root = ET.fromstring(data)
for element in root.iter():
   # 简单地访问.text属性，因为在大多数情况下解析器已经处理好CDATA了
   if element.text:
      print(element.text)

# 方法二：字符串混合xpath处理
datastore = etree.HTML(data)
for row in datastore.xpath('//record'):
   record = etree.tostring(row).replace(r'<record>', '').replace(r']]&gt;</record>', '')
   record1 = etree.HTML(record)
   print etree.tostring(record1)
"""
```

##### 代码中执行单个爬虫

```python
from scrapy import cmdline
cmdline.execute("scrapy crawl xxx -o ./data/xxx.csv -s LOG_FILE=./logs/xxx.log".split())
```

#### Selenium

##### Selenium 去掉爬虫标识（与之前不同）

```python
options = webdriver.ChromeOptions()
options.add_argument("--disable-blink-features=AutomationControlled")  # 将window.navigator.webdriver设置为false
browser = webdriver.Chrome(options=options)


#=============== 之前是这样，没好使？ =====================
options = webdriver.ChromeOptions()
# selenium 不显示正在被测试软件控制
options.add_experimental_option('excludeSwitches', ['enable-automation'])
options.add_experimental_option('useAutomationExtension', False)
browser = webdriver.Chrome(options=options)
# 防止通过变量识别到 selenium
browser.execute_cdp_cmd(
    'Page.addScriptToEvaluateOnNewDocument',
    {'source': 'Object.defineProperty(navigator, "webdriver, {get: () => undefined})'}
)
```

##### Selenium 设置请求头

```python
browser.execute_cdp_cmd("Network.enable", {})
browser.execute_cdp_cmd("Network.setExtraHTTPHeaders", {"headers": {"User-Agent": "browser1"}})
```

##### Selenium 设置页面超时时间

```python
# 设置页面加载和js加载超时时间，超时立即报错，如下设置超时时间为x秒
browser.set_page_load_timeout(10)
browser.set_script_timeout(10)
```

##### 爬虫自测网站
```python
https://bot.sannysoft.com/
```


#### 附：代码示例

##### 爬虫代码

```python
from xxx.items import gpItems
from scrapy.spiders import CrawlSpider, Rule
from scrapy.http import Request

import logging
import base64
from Crypto.Cipher import AES

class XXXSpider(CrawlSpider):
	name = 'XXX'
	allowed_domains = ['www.xxx.com']

	# key：数据接口地址   value：source type_code 一次爬取的页面数
	categoryCodeDict = {
		'网址1{}': '描述1 ID1 1 3',  # 17页    15-31页
		'网址2{}': '描述2 ID2 1 6',  # 179页
	}
	headers = {
	}

	def start_requests(self):
		for key, value in self.categoryCodeDict.items():
			start_page = int(value.split(' ')[2])
			page = int(value.split(' ')[3])
			for index in range(start_page, page):
				url = key.format(str(index))
				logging.info('爬取总页数：{}  页面链接：{}'.format(page - 1, url))
				yield Request(url=url, headers=self.headers, method='POST', callback=self.parse, meta={'categoryCode': key, 'categoryText': value})

	def parse(self, response):
		categoryText1 = response.meta['categoryText'].split(' ')[0]
		categoryText2 = response.meta['categoryText'].split(' ')[1]

		rows = response.xpath('//ul[@class="article-list-a"]/li')
		for row in rows:
			i = gpItems()
			i['publishTime'] = row.xpath('div[1]/div[@class="list-times"]/text()').extract_first().strip()
			url = row.xpath('div[1]/a/@href').extract_first().strip()
			ccc = url.split('/')[-1].split('.')[0]
			ddd = self.aes_encode(ccc, '公钥编码').replace('/', '^').replace("==", "")
			i['url'] = url.replace(ccc, ddd)
			i['title'] = ''.join(row.xpath('div[1]/a/text()').extract()).strip()
			ad_name = row.xpath('div[1]/a/label/text()').extract_first()
			if ad_name:
				ad_name = ad_name.strip().replace('【', '').replace('】', '')
				i['ad_name'] = 'sss' if ad_name == 'bbb' else ad_name
			else:
				i['ad_name'] = 'xxx'
			i['source'] = categoryText1
			i['type_code'] = categoryText2
			yield Request(i['url'], headers=self.headers, meta={'item': i}, callback=self.parse_item)

	def parse_item(self, response):
		i = response.meta['item']
		i['htmlcontent'] = response.xpath('//div[@class="class1111"]').extract_first().strip()
		return i

	def aes_encode(self, data, key):
		global aes
		while len(data) % 16 != 0:  # 补足字符串长度为16的倍数
			data += (16 - len(data) % 16) * chr(16 - len(data) % 16)
			data = str.encode(data)
			aes = AES.new(str.encode(key), AES.MODE_ECB)  # 初始化加密器
		return str(base64.encodebytes(aes.encrypt(data)), encoding='utf8').replace('\n', '')  # 加密
```

##### 中间件使用动态代理IP测试

```python
class AutotestDownloaderMiddleware(object):
    delay_sec = [0.1, 0.2, 0.3, 0.4, 0.5]   # , 0.6, 0.7, 0.8, 0.9
    proxy_ip_dict = {}
    # 需要走代理的爬虫
    spiders_list = ['XXX']

    def spider_opened(self, spider):
        # 初始化代理池
        if spider.name in self.spiders_list:
            while len(self.proxy_ip_dict) < 6:
                time.sleep(5)
                self.get_proxy()

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        if spider.name in self.spiders_list:
            # time.sleep(random.choice(self.delay_sec))
            proxy_ip = self.determine(int(time.time()*1000))
            print('request.meta[\'proxy\'] = {proxy_ip}'.format(proxy_ip=proxy_ip))
            print('proxy_ip_list：{}'.format(self.proxy_ip_dict))
            request.meta['proxy'] = proxy_ip

    def process_response(self, request, response, spider):
        return response

    def process_exception(self, request, exception, spider):
        pass

    def get_proxy(self, proxy_pop=''):
        curtime = int(time.time()*1000)
        api_url = '购买的动态IP接口'   # 返回值为 IP:PORT,可用时间(毫秒)
        response = requests.get(api_url).text.strip()
        proxy = 'http://' +response.split(',')[0]
        mill = int(response.split(',')[1])
        print('从api获取动态ip：{}'.format(response))
        # self.proxy_ip_list.append((proxy, curtime + mill))
        # 已经有,更新；没有,删除旧的,插入新的   proxy_pop为空字符串时初始化字典
        if proxy_pop == '' or self.proxy_ip_dict.get(proxy) is not None:
            pass
        else:
            print("==================删除ip======{}".format(proxy_pop))
            self.proxy_ip_dict.pop(proxy_pop)
        self.proxy_ip_dict[proxy] = curtime + mill - 2000 # 2s预留时间差
        # return (proxy, curtime + mill)
        return proxy

    def determine(self, curmill):
        # 获取ip
        item1 = random.choice(list(self.proxy_ip_dict.keys()))
        print('可用时间：{}  当前时间：{}'.format(self.proxy_ip_dict[item1], curmill))
        if self.proxy_ip_dict[item1] > curmill:
            # 若可用，返回
            # return tuple1[0]
            return item1
        else:
            print("================== ip：{}  过期时间：{}  当前时间：{} ======".format(item1, self.proxy_ip_dict[item1], int(time.time()*1000) ))
            # 若不可用，从列表移除  重新获取
            return self.get_proxy(item1)
```

##### 中间件使用Selenium发送请求 + 动态代理IP


```python
from selenium import webdriver
from scrapy.downloadermiddlewares.retry import get_retry_request

class SeleniumDownloaderMiddleware(object):
    # 需要走代理的爬虫
    spiders_list = ['xxxx', 'xxxxxx']
    proxy_ip_dict = {}  # key：代理ip  value：最大可用毫秒数 使用次数 错误次数（selenium中获取不到代理IP，故同一时间只能使用一个代理IP）
    proxy = False    # 是否走代理

    def __init__(self, timeout=50):
        self.browser = None

    def spider_opened(self, spider):
        if spider.name in self.spiders_list:
            # 创建浏览器对象
            self.browser = self.create_chrome_driver()
            print("创建浏览器对象")
            if self.proxy:
                while len(self.proxy_ip_dict) == 0:   # 每次只有一个IP
                    self.get_proxy()

    def spider_closed(self, spider):
        if spider.name in self.spiders_list and self.browser:
            print(self.browser)
            self.browser.close()
            print(self.browser)
            print("关闭浏览器对象")

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        # 绑定 spider_closed 信号的处理方法
        crawler.signals.connect(s.spider_closed, signal=signals.spider_closed)
        return s

    def process_request(self, request, spider):
        if spider.name in self.spiders_list:
            if self.proxy:
                self.preparatory_work(int(time.time()*1000))
            self.browser.get(request.url)
            time.sleep(random.uniform(0.1, 0.5))
            print("=" * 100)
            print("#########{}".format(self.browser.current_url))
            return HtmlResponse(url=request.url, body=self.browser.page_source, encoding="utf-8",
                                request=request)  # 然后把这个response对象返回给爬虫  self.browser.current_url

    def process_response(self, request, response, spider):
        if spider.name in self.spiders_list:
            print("====================== process_response ====================================")
            print(response.status)
            if self.proxy:
                # 使用 next() 函数获取第一个键值对
                first_key, first_value = next(iter(self.proxy_ip_dict.items()))
                item_info = self.proxy_ip_dict[first_key].split(' ')
                item_info[1] = str(int(item_info[1]) + 1)  # 使用次数+1
                if response.status != 200:
                    item_info[2] = str(int(item_info[2]) + 1)  # 错误次数+1
                self.proxy_ip_dict[first_key] = ' '.join(item_info)
        # 则返回原始的响应对象
        return response

    def process_exception(self, request, exception, spider):
        if spider.name in self.spiders_list:
            print("====================== process_exception ====================================")
            print(request.url)
            print(exception)
            if self.proxy:
                # 使用 next() 函数获取第一个键值对
                first_key, first_value = next(iter(self.proxy_ip_dict.items()))
                item_info = self.proxy_ip_dict[first_key].split(' ')
                item_info[1] = str(int(item_info[1]) + 1)  # 使用次数+1
                item_info[2] = str(int(item_info[2]) + 1)  # 错误次数+1
                self.proxy_ip_dict[first_key] = ' '.join(item_info)
            request.meta['dont_filter'] = True    # 可以不加，retry函数中会修改
            request.meta['max_retry_times'] = 4   # 增加重试次数
            return get_retry_request(request=request, spider=spider, reason='process_exception')
        pass

    #=========================================== 自定义函数 =====================================
    def create_chrome_driver(self, headless=False, proxy_ip=None):
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
            options.add_argument('--disable‐gpu')
            # headless 模式请求头与正常不一致，需修改请求头
            options.add_argument(f'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.79 Safari/537.36')
        else:
            options.add_argument("--start-maximized")
        if proxy_ip: options.add_argument('--proxy-server={}'.format(proxy_ip))

        options.add_argument("--disable-blink-features=AutomationControlled")  # 将window.navigator.webdriver设置为false
        options.add_experimental_option("prefs", {"profile.mamaged_default_content_settings.images": 2})  # 不加载图片,加快访问速度
        # selenium 不显示正在被测试软件控制
        options.add_experimental_option('excludeSwitches', ['enable-automation'])
        options.add_experimental_option('useAutomationExtension', False)
        browser = webdriver.Chrome(options=options)
        # 防止通过变量识别到 selenium
        browser.execute_cdp_cmd(
            'Page.addScriptToEvaluateOnNewDocument',
            {'source': 'Object.defineProperty(navigator, "webdriver, {get: () => undefined})'}
        )

        # browser.execute_cdp_cmd("Network.enable", {})
        # browser.execute_cdp_cmd("Network.setExtraHTTPHeaders", {"headers": {"User-Agent": "browser1"}})

        # 设置页面加载和js加载超时时间，超时立即报错，如下设置超时时间为x秒
        browser.set_page_load_timeout(10)
        browser.set_script_timeout(10)
        time.sleep(2)
        print("=========== 创建浏览器对象，代理IP为{} ================".format(proxy_ip))
        return browser

    def get_proxy(self, proxy_pop=''):
        api_url = '代理商API接口'
        curtime, proxy, mill = None, None, None
        for i in range(3):
            if i > 0: time.sleep(3)
            curtime = int(time.time()*1000)
            response = requests.get(api_url).text.strip()
            proxy = 'http://' + response.split(',')[0]
            mill = int(response.split(',')[1])
            if self.proxy_ip_dict.get(proxy) is None: break   # 获取到新的，跳出循环
        # proxy_pop为空字符串时初始化字典
        if proxy_pop == '':
            self.proxy_ip_dict[proxy] = '{} 0 0'.format(curtime + mill)
            if self.browser:
                self.browser.close()
            self.browser = self.create_chrome_driver(proxy_ip=proxy)
        # IP已经存在,跳过
        elif self.proxy_ip_dict.get(proxy) is not None:
            pass
        # 删除旧的,插入新的
        else:
            print("==================删除ip======{}".format(proxy_pop))
            self.proxy_ip_dict.pop(proxy_pop)
            self.proxy_ip_dict[proxy] = '{} 0 0'.format(curtime + mill)
            if self.browser:
                self.browser.close()
            self.browser = self.create_chrome_driver(proxy_ip=proxy)
        return proxy

    def preparatory_work(self, curmill):
        proxy_ip = random.choice(list(self.proxy_ip_dict.keys()))
        item_info = self.proxy_ip_dict[proxy_ip].split(' ')
        print('可用时间（使用次数、错误次数）：{}  当前时间：{}'.format(self.proxy_ip_dict[proxy_ip], curmill))
        if int(item_info[0]) > curmill and int(item_info[2]) == 0:  # and int(item_info[1]) < 10
            return proxy_ip  # 若可用，返回
        else:
            print("================== ip：{}  过期时间：{}  当前时间：{} ======".format(proxy_ip, item_info[0], int(time.time()*1000)))
            return self.get_proxy(proxy_ip)   # 若不可用，从列表移除  重新获取
```