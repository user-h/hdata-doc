



#### 琐碎代码

##### 程序中主动退出爬虫

```python
# scrapy框架中手动退出爬虫程序
self.crawler.engine.close_spider(self, '验证码尝试三次不正确，退出爬虫')
```

##### 日期格式化

```python
# pip install python-dateutil
import datetime
from dateutil import parser

time_string = "Fri Jan 05 15:41:17 GMT+08:00 2024"
dt_object = parser.parse(time_string)
print(dt_object.strftime("%Y-%m-%d %H:%M:%S"))
# 转换为 UTC 时间
print(dt_object.astimezone(tz=datetime.timezone.utc))
# 或者如果你想转换到本地时区（假设你的系统配置了正确的本地时区）
print(dt_object.astimezone())
```

##### 发送请求并获取请求头中的Cookie

```python
response = requests.get(url, headers=headers)
self.cookies = requests.utils.dict_from_cookiejar(response.cookies)
```

##### json 转字符串乱码解决

```python
# 确保输出的结果中不包含非ASCII字符
json.dumps(response["result"], ensure_ascii=False)
```

##### 使用 lxml 解析数据

```python
data="""
<datastore>
<recordset>
<record><![CDATA[
<div class="fyxx_lsbox">	<div class="btqz"></div>	<div class="fyxx_lsbox_r">		<a title="528国道龙游十里铺至上圩头段改建工程竣（交）工质量评定检测竣（交）工质量评定检测[E3300000007000941003001]招标公告" target="_blank" href="/art/2024/1/12/art_1229682709_457018.html">528国道龙游十里铺至上圩头段改建工程竣（交）工质量评定检测竣（交）工质量评定检测[E3300000007000941003001]招标公告</a> <span>2024-01-12</span> </div></div>]]></record>
</recordset>
</datastore>

# 方法一：工具直接解析（上述数据不太符合，但写法应如此）
from xml.etree import ElementTree as ET
root = ET.fromstring(data)
for element in root.iter():
   # 简单地访问.text属性，因为在大多数情况下解析器已经处理好CDATA了
   if element.text:
      print(element.text)

# 方法二：字符串混合xpath处理
datastore = etree.HTML(data)
for row in datastore.xpath('//record'):
   record = etree.tostring(row).replace(r'<record>', '').replace(r']]&gt;</record>', '')
   record1 = etree.HTML(record)
   print etree.tostring(record1)
"""
```

##### 代码中执行单个爬虫

```python
from scrapy import cmdline
cmdline.execute("scrapy crawl xxx -o ./data/xxx.csv".split())
```


#### 附：代码示例

```python
from xxx.items import gpItems
from scrapy.spiders import CrawlSpider, Rule
from scrapy.http import Request

import logging
import base64
from Crypto.Cipher import AES

class XXXSpider(CrawlSpider):
	name = 'XXX'
	allowed_domains = ['www.xxx.com']

	# key：数据接口地址   value：source type_code 一次爬取的页面数
	categoryCodeDict = {
		'网址1{}': '描述1 ID1 1 3',  # 17页    15-31页
		'网址2{}': '描述2 ID2 1 6',  # 179页
	}
	headers = {
	}

	def start_requests(self):
		for key, value in self.categoryCodeDict.items():
			start_page = int(value.split(' ')[2])
			page = int(value.split(' ')[3])
			for index in range(start_page, page):
				url = key.format(str(index))
				logging.info('爬取总页数：{}  页面链接：{}'.format(page - 1, url))
				yield Request(url=url, headers=self.headers, method='POST', callback=self.parse, meta={'categoryCode': key, 'categoryText': value})

	def parse(self, response):
		categoryText1 = response.meta['categoryText'].split(' ')[0]
		categoryText2 = response.meta['categoryText'].split(' ')[1]

		rows = response.xpath('//ul[@class="article-list-a"]/li')
		for row in rows:
			i = gpItems()
			i['publishTime'] = row.xpath('div[1]/div[@class="list-times"]/text()').extract_first().strip()
			url = row.xpath('div[1]/a/@href').extract_first().strip()
			ccc = url.split('/')[-1].split('.')[0]
			ddd = self.aes_encode(ccc, '公钥编码').replace('/', '^').replace("==", "")
			i['url'] = url.replace(ccc, ddd)
			i['title'] = ''.join(row.xpath('div[1]/a/text()').extract()).strip()
			ad_name = row.xpath('div[1]/a/label/text()').extract_first()
			if ad_name:
				ad_name = ad_name.strip().replace('【', '').replace('】', '')
				i['ad_name'] = 'sss' if ad_name == 'bbb' else ad_name
			else:
				i['ad_name'] = 'xxx'
			i['source'] = categoryText1
			i['type_code'] = categoryText2
			yield Request(i['url'], headers=self.headers, meta={'item': i}, callback=self.parse_item)

	def parse_item(self, response):
		i = response.meta['item']
		i['htmlcontent'] = response.xpath('//div[@class="class1111"]').extract_first().strip()
		return i

	def aes_encode(self, data, key):
		global aes
		while len(data) % 16 != 0:  # 补足字符串长度为16的倍数
			data += (16 - len(data) % 16) * chr(16 - len(data) % 16)
			data = str.encode(data)
			aes = AES.new(str.encode(key), AES.MODE_ECB)  # 初始化加密器
		return str(base64.encodebytes(aes.encrypt(data)), encoding='utf8').replace('\n', '')  # 加密
```