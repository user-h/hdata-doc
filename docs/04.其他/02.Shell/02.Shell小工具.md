---
title: Shell命令行
date: 2022-02-27 15:35:17
permalink: /pages/5765cc/
categories: 
  - 其他
  - Shell
tags: 
  - 
---


## 常用命令

#### 批量移动指定文件

将文件粘贴到 selected.txt 中，使用 Unix 换行符号

```shell
for file in $(cat selected.txt); do mv "$file" selected/; done
```

#### 批量移动/拷贝文件

```shell
# （递归）查找aaa目录下所有jpg、pdf、png文件，并拷贝到bbb目录
find ./aaa -type f \( -iname '*.jpg' -o -iname '*.pdf' -o -iname '*.png' \) -exec cp {} bbb \;
```

#### 批量杀死进程

```shell
ps -ef|grep java|grep -v grep|awk '{print $2}'|xargs kill
```

#### 查看目录大小

```shell
# 查看当前目录及下级目录大小，按大小排序
du -h --max-depth=1 . | sort -hr
# 查看当前目录下所有文件和目录的详细大小
du -ah --max-depth=1 | sort -hr
```

#### 删除当前目录下指定用户（所有者）的所有文件

```shell
# 预览
sudo find . -user root -ls
# sudo find . \( -path ./topic1-0 -o -path ./topic2-0 \) -user root -ls  # 指定目录？？？
# 删除（确认后）
sudo find . -user root -delete
# 仅删除文件
sudo find . -type f -user root -delete
# 仅删除目录
sudo find . -type d -user root -delete
```

## 工具

---
### 日志处理

```shell script
#!/bin/bash
set -e

if [ ! -n "$1" ];then
        pre_date=`date -d "-1 day now" +%Y-%m-%d`
else
        pre_date=`date -d "$1" +%Y-%m-%d`
fi
echo "===============处理日志的日期为========================="$pre_date


parent_dir=`date -d "$pre_date" +%Y-%m`
path="/pp/pp-etl-1.0/log/$parent_dir/$pre_date"
reg1="进度[0-9]{3,}\.[0-9]{2}%"
reg2=".*([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}).*\[INFO\] ([a-z|A-Z]+)，(.*_V1).*插入([0-9]+).*更新([0-9]+).*进度([0-9]{3,}\.[0-9]{2})%.*更新至([0-9]{4}-[0-9]{2}-[0-9]{2})。"
file="/home/xxx/tag.sql"


# 判断目录存不存在
echo "=========日志路径为 $path=========="
if [ ! -d "$path" ];then
	echo "该日期目录不存在，退出程序..."
	exit 1
fi

if [ `ls $path/custom*.log|wc -l` -eq 0 ];then
	echo "没有需要的日志文件，退出程序..."
	exit 1
else
	grep -E $reg1 "$path/"custom*.log | sed -E "s#$reg2#insert into xxx(xx, xx, xx, xx, xx, xx, xx) values('\1','\2','\3',\4,\5,\6,'\7');#g" > "$file"
fi

# 判断文件是否存在
if [ ! -f "$file" ];then
	exit 1
fi
# 双引号替换为单引号
sed -i "s/\"/'/g" "$file"

# 取文件名 -Djava.security.egd=file:/dev/random 
java -Dfile=tag.sql -jar xxx.jar
if [ $? -ne 0 ];then
	echo "执行失败,退出..."
else
	#rm -f "$file"
	echo "执行成功，退出..."
fi
```

###  集群脚本

启动集群
```shell script
#!/bin/bash
# 获取当前用户名称
user=`whoami`
source /etc/profile
echo ===================集群开始启动====================
echo -------------------启动zk节点---------------------
for((host=101; host<=103; host++)); do
        echo ------------------- hadoop$host --------------
        ssh $user@hadoop$host '/soft/module/zookeeper-3.5.7/bin/zkServer.sh start'
done
echo -------------------启动dfs集群-------------
ssh $user@hadoop101 '/soft/module/hadoop-2.9.2/sbin/start-dfs.sh'
echo -------------------启动hbase集群-----------
ssh $user@hadoop100 '/soft/module/hbase/bin/start-hbase.sh'
echo -------------------启动hbase备份master--------------
ssh $user@hadoop101 '/soft/module/hbase/bin/hbase-daemon.sh start master'
```

停止集群
```shell script
#!/bin/bash
# 获取当前用户名称
user=`whoami`
source /etc/profile
echo ===================集群开始关闭====================
echo -------------------关闭hbase备份master--------------
ssh $user@hadoop101 '/soft/module/hbase/bin/hbase-daemon.sh stop master'
echo -------------------关闭hbase集群-----------
ssh $user@hadoop100 '/soft/module/hbase/bin/stop-hbase.sh'
echo -------------------关闭dfs集群-------------
ssh $user@hadoop101 '/soft/module/hadoop-2.9.2/sbin/stop-dfs.sh'
echo -------------------关闭zk节点---------------------
for((host=101; host<=103; host++)); do
        echo ------------------- hadoop$host --------------
        ssh $user@hadoop$host '/soft/module/zookeeper-3.5.7/bin/zkServer.sh stop'
done
```

jps查看进程
```shell script
#!/bin/bash
# 获取当前用户名称
user=`whoami`
for((host=100; host<=103; host++)); do
        echo ------------------- hadoop$host --------------
        ssh $user@hadoop$host '/soft/module/jdk1.8.0_161/bin/jps'
done
```

::: tip
shell脚本执行的过程中，登录shell 和非登录shell 读取的环境配置文件不同。
登录shell 会读取 /etc/profile, ~/.bash_profile,~/.bash_login ,~/.profile等文件，而非登录shell 读取的脚本有 /etc/bashrc 和 ~/.bashrc ，像java，path 这些环境变量读取不到。
解决方法：
1.将/etc/profile文件写入 ~/.bashrc ;cat /etc/profile >~/.bashrc
2.把profile的配置信息echo到.bashrc中echo 'source /etc/profile' >> ~/.bashrc

:::


### 服务器搭建(tomcat)

环境变量
```shell script
## java
JAVA_HOME=/soft/jdk1.8.0_161
CLASSPATH=%JAVA_HOME%/lib:%JAVA_HOME%/jre/lib
PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin

## maven
export MAVEN_HOME=/soft/apache-maven-3.6.3
export PATH=$MAVEN_HOME/bin:$PATH

## tomcat
CATALINA_HOME=/soft/apache-tomcat-9.0.41
```

tomcat
```shell script
sh startup.sh（启动tomcat命令）
关闭tomcat命令：
sh /usr/local/tomcat7/bin/shutdown.sh

ps -ef|grep java
查询端口是否有进程守护用如下命令grep对应端口，如8088为端口号
例：netstat -nalp|grep 8080

#停止firewall
systemctl stop firewalld.service 
 
#开启firewall
systemctl start firewalld.service
 
#禁止firewall开机启动
systemctl disable firewalld.service 
 
#查看默认防火墙状态（关闭后显示not running，开启后显示running）
firewall-cmd --state

开放指定端口
firewall-cmd --zone=public --add-port=8080/tcp --permanent
 命令含义：
--zone #作用域
--add-port=1935/tcp  #添加端口，格式为：端口/通讯协议
--permanent  #永久生效，没有此参数重启后失效

可以通过"netstat -anp" 来查看哪些端口被打开
然后可以通过"lsof -i:$PORT"查看应用该端口的程序（$PORT指对应的端口号）。或者你也可以查看文件/etc/services，从里面可以找出端口所对应的服务。
```

Linux Tomcat启动慢
```shell script
方案1：通过rng-tools自动补充熵池（推荐）

安装rng服务并启动，然后修改它的配置文件
yum install rng-tools -y
systemctl start rngd
cp /usr/lib/systemd/system/rngd.service /etc/systemd/system
cd /etc/systemd/system/

vim rngd.service 
将 ExecStart=/sbin/rngd -f 改为 ExecStart=/sbin/rngd -f -r /dev/urandom
重新加载后，再重启服务
systemctl daemon-reload  
systemctl restart rngd
```

### sed替换json文件

```shell script
##########准备参数########################
filename=`cat "json.properties"|grep 'filename'|awk -F '==' '{print $2}'`          #字段文件名
parameter_name=`cat "json.properties"|grep 'parameter_name'|awk -F '==' '{print $2}'`     #分区参数
mysql_col=`cat "json.properties"|grep 'mysql_col'|awk -F '==' '{print $2}'`             #mysql对应字段名
hdfs_table=`cat "json.properties"|grep 'hdfs_table'|awk -F '==' '{print $2}'`                   #hdfs表名
mysql_table=`cat "json.properties"|grep 'mysql_table'|awk -F '==' '{print $2}'`                 #mysql对应表名
hdfs_path=`cat "json.properties"|grep 'hdfs_path'|awk -F '==' '{print $2}'`

echo "$filename===$parameter_name===$mysql_col===$hdfs_table===$mysql_table===${hdfs_path}"

########拼接字符串####################
sh create_column.sh $filename $parameter_name $mysql_col #"字段文件名" "分区参数" "mysql分区字段"
filename="hdfs_mysql_${hdfs_table}.json"
########替换文件#########################
cp "template.json" "json/${filename}"
cd json
sed -i "s#hdfscolumns#$(cat '../hdfscolumn')#g" $filename
sed -i "s#mysqlcolumns#$(cat '../mysqlcolumn')#g" $filename
sed -i "s#hdfspath#${hdfs_path}#g" $filename
sed -i "s#mysqltablename#${mysql_table}#g" $filename
sed -i "s#parameter_name#${parameter_name}#g" $filename
sed -i "s#mysqlcol#${mysql_col}#g" $filename
cd -


rm -f "hdfscolumn"
rm -f "mysqlcolumn"
for i in $(cat $1)
do
        array=(${i//|/ })
        #`echo ${array[0]}
        # echo ${array[1]}
        echo -n "{\"index\" : \"${array[0]}\",\"type\" : \"${array[1]}\"}," >> hdfscolumn

        echo -n "\"${array[2]}\"," >> mysqlcolumn
done

echo -n "{\"value\" : \"$2\",\"type\" : \"string\"}" >> hdfscolumn

echo -n "\"$3\"" >> mysqlcolumn
```

### awk按某一列把文件分成多个文件

```shell script
sed 's/\t/\x01/g' "data.csv" > "new_data.txt"
for idate in 2020-12-08 2020-12-07
do
    awk 'BEGIN{FS="\\\x01";var="'${idate}'";re="^'${idate}'"} $3~re{print $1"\t"$2"\t"$3>var".txt"}' new_data.csv
    # hdfs dfs -put
done
```

```shell script
# awk 获取某列长度大于20的行(筛选)
awk -F, '{if(length($1)>20) print $0}'  xxx.csv
```

### 替换文本内容(sed替换的文本不能有命令分隔符)

```shell script
for i in $(cat filename.txt)
do
        array=(${i//,/ })
        #`echo ${array[0]}
        # echo ${array[1]}
        cp template.sh 1216/${array[1]}
        sed -i "s#httpsed#${array[0]}#g" 1216/${array[1]}
done

##
for filename in $(cat "filename.txt")
do
    #echo $filename
    filename_temp=${filename/"dwa"/"cio"}
    echo $filename_temp
    mv $filename $filename_temp
    sed -i 's/dwa/cio/g' $filename_temp
done

##
for CLASSIFY_DESC in 1002 1003 1004 1005 1006 1007 1008 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 3001 3002 3003 3004 3007 3008 3009 3010 3011 4002 4003 4004 4005 4006 4007
do
   filename1="cio_cp_ctyun_website_index_day_1001_1.0.0.0.sql"
   filename2=${filename1/1001/"${CLASSIFY_DESC}"}
   echo $filename2
   cp ${filename1} ${filename2}
   sed -i s/CLASSIFY_DESC/"${CLASSIFY_DESC}"/g ${filename2} 
done
```

### 计算时间

```shell script
input_date_time=$(date -d "2020-10-23 03:48:25" '+%Y-%m-%d %H:%M:%S')
MONTH_ID=${input_date_time:0:7}
PRE_MONTH_ID=$(date -d "${MONTH_ID}-01  -1 month" +%Y-%m)

PRE_2MONTH_ID=$(date -d "${MONTH_ID}-01  -2 month" +%Y-%m)
PRE_3MONTH_ID=$(date -d "${MONTH_ID}-01  -3 month" +%Y-%m)
PRE_4MONTH_ID=$(date -d "${MONTH_ID}-01  -4 month" +%Y-%m)
PRE_5MONTH_ID=$(date -d "${MONTH_ID}-01  -5 month" +%Y-%m)
NEXT_MONTH=$(date -d "${MONTH_ID}-01  +1 month" +%Y-%m)
LAST_DAY_MONTH=$(date -d "${NEXT_MONTH}-01 -1 days" +%Y-%m-%d)
FIRST_DAY_MONTH=$(date -d "${MONTH_ID}-01" +%Y-%m-%d)
FIRST_DAY_PRE_MONTH=$(date -d "${MONTH_ID}-01  -1 month" +%Y-%m-%d)
FIRST_DAY_PRE_2MONTH=$(date -d "${MONTH_ID}-01  -2 month" +%Y-%m-%d)
FIRST_DAY_PRE_3MONTH=$(date -d "${MONTH_ID}-01  -3 month" +%Y-%m-%d)
FIRST_DAY_PRE_4MONTH=$(date -d "${MONTH_ID}-01  -4 month" +%Y-%m-%d)

echo "PRE_MONTH_ID : ${PRE_MONTH_ID}"
echo "PRE_2MONTH_ID : ${PRE_2MONTH_ID}"
echo "PRE_3MONTH_ID : ${PRE_3MONTH_ID}"
echo "PRE_4MONTH_ID : ${PRE_4MONTH_ID}"
echo "PRE_5MONTH_ID : ${PRE_5MONTH_ID}"
echo "LAST_DAY_MONTH : ${LAST_DAY_MONTH}"
echo "FIRST_DAY_MONTH : ${FIRST_DAY_MONTH}"
echo "FIRST_DAY_PRE_MONTH : ${FIRST_DAY_PRE_MONTH}"
echo "FIRST_DAY_PRE_2MONTH : ${FIRST_DAY_PRE_2MONTH}"
echo "FIRST_DAY_PRE_3MONTH : ${FIRST_DAY_PRE_3MONTH}"
echo "FIRST_DAY_PRE_4MONTH : ${FIRST_DAY_PRE_4MONTH}"
```

