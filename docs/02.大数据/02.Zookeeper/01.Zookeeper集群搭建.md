---
title: Zookeeper集群搭建
date: 2022-02-27 15:35:17
permalink: /pages/e2226d/
categories: 
  - 大数据
  - Zookeeper
tags: 
  - 
---
这个是在之前的基础上搭建
**hadoop完全分布式搭建:https://www.cnblogs.com/Hephaestus/p/12213719.html**
**hadoop高可用搭建:https://www.cnblogs.com/Hephaestus/p/12420370.html**
集群规划:

|        | **namenode** | **datanode** |**journalnode** |**Zookeeper(必须为奇数)** |
| :----: | :----: | :----: |:----: |:----: |
|hadoop100| 是(nn2) |      |    |    |
|hadoop101| 是(nn1) |  是  |是|是|
|hadoop102|         |  是  |是|是|
|hadoop103|         |  是  |是|是|

## 关于Zookeeper的节点为什么是奇数
**(参考链接:https://www.cnblogs.com/ysocean/p/9860529.html#_label0)**
- **容错率**
需要保证集群能够有半数进行投票
>2台服务器，至少2台正常运行才行（2的半数为1，半数以上最少为2），正常运行1台服务器都不允许挂掉，但是相对于 单节点服务器，2台服务器还有两个单点故障，所以直接排除了。
3台服务器，至少2台正常运行才行（3的半数为1.5，半数以上最少为2），正常运行可以允许1台服务器挂掉
4台服务器，至少3台正常运行才行（4的半数为2，半数以上最少为3），正常运行可以允许1台服务器挂掉
5台服务器，至少3台正常运行才行（5的半数为2.5，半数以上最少为3），正常运行可以允许2台服务器挂掉
- **防脑裂**
脑裂集群的脑裂通常是发生在节点之间通信不可达的情况下，集群会分裂成不同的小集群，小集群各自选出自己的leader节点，导致原有的集群出现多个leader节点的情况，这就是脑裂。
>3台服务器，投票选举半数为1.5，一台服务裂开，和另外两台服务器无法通行，这时候2台服务器的集群（2票大于半数1.5票），所以可以选举出leader，而 1 台服务器的集群无法选举。
4台服务器，投票选举半数为2，可以分成 1,3两个集群或者2,2两个集群，对于 1,3集群，3集群可以选举；对于2,2集群，则不能选择，造成没有leader节点。
5台服务器，投票选举半数为2.5，可以分成1,4两个集群，或者2,3两集群，这两个集群分别都只能选举一个集群，满足zookeeper集群搭建数目。

以上分析，我们从容错率以及防止脑裂两方面说明了3台服务器是搭建集群的最少数目，4台发生脑裂时会造成没有leader节点的错误

## 1. 解压安装:
下载地址: 
**解压: `tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /soft/module/` (自己要安装的目录)**
**文件夹重命名: `mv apache-zookeeper-3.5.7 zookeeper-3.5.7`**

## 2.	配置
- **1)	进入到zookeeper-3.5.7/conf目录下 `cp zoo_sample.cfg zoo.cfg` 编辑 `vim zoo.cfg`,加入**
```
dataDir=/soft/module/zookeeper-3.5.7/zkData
dataLogDir=/soft/module/zookeeper-3.5.7/logs
clientPort=2181

########hadoop101、102、103为 3个datanode节点########
# 注: 文件可以这样写(不用0.0.0.0)(前提是把/etc/hosts前两行注释掉,否则会产生冲突)
server.1=hadoop101:2888:3888
server.2=hadoop102:2888:3888
server.3=hadoop103:2888:3888
```
**分发文件 `xsync.sh zookeeper-3.5.7`**
- **进入到zookeeper-3.5.7/conf目录下 ,`mkdir zkData`(上面配置文件zoo.cfg的路径)  `echo "1" > myid`写入内容1(注意和配置文件中的server.1对应)**
    **在其它节点分别执行操作 `echo "2" > myid` `echo "3" > myid`(和zoo.cfg对应!!!  id必须在集群环境中服务器标识中是唯一的，且大小在1～255之间)**

## 3.	集群启动 测试服务
- **启动 `bin/zkServer.sh start `**
- **查看状态 `bin/zkServer.sh status`**
![](https://img2020.cnblogs.com/blog/1798447/202003/1798447-20200305171334884-1216608613.png)
这种情况的一种可能是Zookeeper集群有节点未启动
- **连接zookeeper服务 `bin/zkCli.sh -server 8.8.8.103:2181` (自己的ip、端口号和zoo.cfg对应)**

>拒绝连接
1).防火墙没关
2).配置文件问题
3).zk集群不是正常的关闭 (例如执行kill 命令zk的进程)
解决: 直接修改clientPort端口号，然后再启动，再关闭，把clientPort修改回来
注意，使用zookeeper需正常停止！！！不然，重启linux都不一定能解决问题！
4).主机IP和端口号一定要正确!!! (我就是端口号错了…)

## 4.	HA故障自动转换
- ### 1)	修改集群所有节点(NameNode和DataNode)的hdfs配置文件和核心配置文件:
    `vim /soft/module/hadoop-2.9.2/etc/hadoop/hdfs-site.xml`
```
<!-- 当namnode故障，是否自动启动另一个namenode(默认值为false)-->
<property>
		<!--这个之前好像错了 -->
        <name>dfs.ha.automatic-failover.enabled </name>
        <value>true</value>
</property>
```
`vim /soft/module/hadoop-2.9.2/etc/hadoop/core-site.xml	`
```
<!-- 指定zookeeper地址 -->
<property>
    <name>ha.zookeeper.quorum</name>
    <!-- 端口号和配置文件zoo.cfg 对应 -->
    <value>hadoop101: 2181,hadoop102: 2181,hadoop103: 2181</value>
</property>
```
查看 `mapred-site.xml` `yarn-site.xml `主机名啥的不要出错
**分发配置文件: `xsync /soft/module/hadoop-2.9.2/etc/hadoop`**
### 2)	启动测试
- **`jps`查看是否关闭所有服务, 没有的话关闭**
- **在所有节点上删除 `dfs/data  dfs/name  tmp logs`目录**
- **所有datanode节点启动zookeeper集群 `./bin/zkServer.sh start`**
- **格式化zk集群: nn1上执行 `bin/hdfs zkfc -formatZK`**
- **所有datanode节点启动journalnode集群 `./sbin/hadoop-daemon.sh start journalnode`**
- **格式化namenode: nn1上执行 `./bin/hdfs namenode -format`**
- **启动datanode和namenode**
    所有datanode节点执行 `./sbin/hadoop-daemon.sh start datanode`
    nn1 执行 `./sbin/hadoop-daemon.sh start namenode`
    nn2 执行 `./bin/hdfs namenode -bootstrapStandby`
            `./sbin/hadoop-daemon.sh start namenode`
此时可以再UI界面看到
![](https://img2020.cnblogs.com/blog/1798447/202003/1798447-20200305172410970-1093843624.png)

- **启动zkfc服务**
    在nn1和nn2执行 `./sbin/hadoop-daemon.sh start zkfc`
    这时namenode1和namenode2会自动选举出active节点, 可以看到
![](https://img2020.cnblogs.com/blog/1798447/202003/1798447-20200305172454651-892827770.png)

- **验证**
    处于active状态的节点 执行 `kill -9 xxxxx `(jps查看的namenode进程id), 可以看到
![](https://img2020.cnblogs.com/blog/1798447/202003/1798447-20200305172534096-750710390.png)
