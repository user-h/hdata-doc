---
title: Spark环境搭建
date: 2022-02-27 17:35:00
permalink: /pages/7d157d/
---
# 集群规划
首先我们要确认我们的Linux主机是否安装了scala，如果没有安装则需要安装，5台机器都需要安装

学习scala时使用的是2.12版本，所以我们选择spark2.4.2及以上的版本



# Spark Standalone模式
## 1.安装使用
### 1）进入spark安装目录下的conf文件夹
`cd spark/conf/`
### 2）修改配置文件名称
`mv slaves.template slaves`
`mv spark-env.sh.template spark-env.sh`
### 3）修改slave文件，添加work节点：
`vim slaves`
```
hadoop101
hadoop102
hadoop103
```
### 4）修改spark-env.sh文件，添加如下配置：
`vim spark-env.sh`
```
export JAVA_HOME=/soft/module/jdk1.8.0_161
export SPARK_MASTER_HOST=hadoop100
export SPARK_MASTER_PORT=7077
```
### 5）分发spark包
`xsync spark/`
### 6）启动(在Master)
`sbin/start-all.sh`
`xcall.sh `
```
------------------- hadoop100 --------------
10021 Jps
9944 Master
------------------- hadoop101 --------------
9159 Jps
9096 Worker
------------------- hadoop102 --------------
8740 Worker
8804 Jps
------------------- hadoop103 --------------
8749 Worker
8813 Jps
```
网页查看：hadoop100:8080
注意：如果遇到 “JAVA_HOME not set” 异常，可以在sbin目录下的spark-config.sh 文件中加入如下配置：
export JAVA_HOME=XXXX
### 7）官方求PI案例
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--executor-memory 1G \
--total-executor-cores 2 \
./examples/jars/spark-examples_2.12-3.0.0-preview2.jar \
100
```
### 8）启动spark shell
```
/soft/module/spark/bin/spark-shell \
--master spark://hadoop100:7077 \
--executor-memory 1g \
--total-executor-cores 2
```
参数：--master spark://hadoop100:7077指定要连接的集群的master
执行WordCount程序
```
scala>sc.textFile("input").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect

res0: Array[(String, Int)] = Array((hadoop,6), (oozie,3), (spark,3), (hive,3), (atguigu,3), (hbase,6))

scala>
```

## 2.JobHistoryServer配置
### 1）修改spark-default.conf.template名称
`mv spark-defaults.conf.template spark-defaults.conf`
### 2）修改spark-default.conf文件，开启Log：
`vim spark-defaults.conf`
```
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://hadoop100:9000/directory
```
注意：HDFS上的目录需要提前存在。
`hadoop fs -mkdir /directory`
### 3）修改spark-env.sh文件，添加如下配置：
`vim spark-env.sh`
```
export SPARK_HISTORY_OPTS="
-Dspark.history.ui.port=18080 
-Dspark.history.retainedApplications=30 
-Dspark.history.fs.logDirectory=hdfs://hadoop100:9000/directory"
```
参数描述：
spark.eventLog.dir：Application在运行过程中所有的信息均记录在该属性指定的路径下； 

spark.history.ui.port=18080  WEBUI访问的端口号为18080

spark.history.fs.logDirectory=hdfs://hadoop102:9000/directory  配置了该属性后，在start-history-server.sh时就无需再显式的指定路径，Spark History Server页面只展示该指定路径下的信息

spark.history.retainedApplications=30指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。
### 4）分发配置文件
`xsync spark-defaults.conf`
`xsync spark-env.sh`
### 5）启动历史服务
`sbin/stop-history-server.sh`
### 6）再次执行任务
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--executor-memory 1G \
--total-executor-cores 2 \
./examples/jars/spark-examples_2.12-3.0.0-preview2.jar \
100
```
### 7）查看历史服务
hadoop100:18080

## 3.HA配置
### 1）zookeeper正常安装并启动
### 2）修改spark-env.sh文件添加如下配置：
`vim spark-env.sh`

注释掉如下内容：
```
#SPARK_MASTER_HOST=hadoop100
#SPARK_MASTER_PORT=7077
```
添加上如下内容：
```
export SPARK_DAEMON_JAVA_OPTS="
-Dspark.deploy.recoveryMode=ZOOKEEPER 
-Dspark.deploy.zookeeper.url=hadoop101,hadoop102,hadoop103 
-Dspark.deploy.zookeeper.dir=/spark"
```
### 3）分发配置文件
`xsync spark-env.sh`
### 4）在hadoop100上(Master)启动全部节点
`sbin/start-all.sh`
### 5）在hadoop101上单独启动master节点
`sbin/start-master.sh`
### 6）spark HA集群访问
```
/soft/module/spark/bin/spark-shell \
--master spark://hadoop100:7077,hadoop102:7077 \
--executor-memory 2g \
--total-executor-cores 2
```
### 7) 执行程序
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://hadoop100:7077,hadoop101:7077 \
--executor-memory 1G \
--total-executor-cores 2 \
./examples/jars/spark-examples_2.12-3.0.0-preview2.jar \
100
```
```
./spark-shell --master spark://hadoop100:7077,hadoop101:7077

hadoop fs -mkdir -p /spark/input
hadoop fs -put RELEASE /spark/input

sc.textFile("/spark/input").flatMap(_.split(" ")).map(word=>(word,1)).reduceByKey(_+_).map(entry=>(entry._2,entry._1)).sortByKey(false,1).map(entry=>(entry._2,entry._1)).saveAsTextFile("/spark/output/")
```
### 附:修改SparkUI界面默认端口号
SparkUI界面默认端口号为8080(可能会被占用,被占用后默认+1HTTP ERROR 404 Not Found),两种方法修改
1. 修改conf/spark-env.sh, 加上`export SPARK_MASTER_WEBUI_PORT=8082`
2. 修改 sbin/start-master.sh
```
if [ "$SPARK_MASTER_WEBUI_PORT" = "" ]; then
  SPARK_MASTER_WEBUI_PORT=8082
fi
```

# Spark Yarn模式
## 1.安装使用
### 1）修改hadoop配置文件yarn-site.xml
添加如下内容：`vim yarn-site.xml`
```
        <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
        <property>
                <name>yarn.nodemanager.pmem-check-enabled</name>
                <value>false</value>
        </property>
        <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
        <property>
                <name>yarn.nodemanager.vmem-check-enabled</name>
                <value>false</value>
        </property>
```
### 2）修改spark-env.sh
添加如下配置：`vim spark-env.sh`
```
YARN_CONF_DIR=/soft/module/hadoop-2.9.2/etc/hadoop
```
### 3）分发配置文件
`xsync /soft/module/hadoop-2.9.2/etc/hadoop/yarn-site.xml`
`xsync spark-env.sh`
### 4）执行一个程序
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
./examples/jars/spark-examples_2.11-2.1.1.jar \
100
```
注意：在提交任务之前需启动HDFS以及YARN集群。
## 2. 日志查看
### 1）修改配置文件spark-defaults.conf
添加如下内容：
```
spark.yarn.historyServer.address=hadoop102:18080
spark.history.ui.port=18080
```
### 2）重启spark历史服务
`sbin/stop-history-server.sh`
```
stopping org.apache.spark.deploy.history.HistoryServer
```
`sbin/start-history-server.sh `
```
starting org.apache.spark.deploy.history.HistoryServer, logging to /opt/module/spark/logs/spark-atguigu-org.apache.spark.deploy.history.HistoryServer-1-hadoop102.out
```
### 3）提交任务到Yarn执行
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
./examples/jars/spark-examples_2.11-2.1.1.jar \
100
```
### 4）Web页面查看日志
`hadoop100:8088`
``

# 附：spark 三种部署模式的区别对比
源网页:https://www.cnblogs.com/eric666666/p/11228825.html

在这三种部署模式中，standalone作为spark自带的分布式部署模式，是最简单也是最基本的spark应用程序部署模式，这里就不再赘述。

这里就讲一下yarn和mesos的区别：

(1) 就两种框架本身而言，mesos上可部署yarn框架。而yarn是更通用的一种部署框架，而且技术较成熟。

(2) mesos双层调度机制，能支持多种调度模式，而Yarn通过Resource　Mananger管理集群资源，只能使用一种调度模式。Mesos 的双层调度机制为：mesos可接入如yarn一般的分布式部署框架，但Mesos要求可接入的框架必须有一个调度器模块，该调度器负责框架内部的任务调度。当一个framework想要接入mesos时，需要修改自己的调度器，以便向mesos注册，并获取mesos分配给自己的资源， 这样再由自己的调度器将这些资源分配给框架中的任务，也就是说，整个mesos系统采用了双层调度框架：第一层，由mesos将资源分配给框架；第二层，框架自己的调度器将资源分配给自己内部的任务。

(3) mesos可实现粗、细粒度资源调度，可动态分配资源，而Yarn只能实现静态资源分配。其中粗粒度和细粒度调度定义如下：

　　粗粒度模式（Coarse-grained Mode）：程序运行之前就要把所需要的各种资源（每个executor占用多少资源，内部可运行多少个executor）申请好，运行过程中不能改变。
　　
　　细粒度模式（Fine-grained Mode）：为了防止资源浪费，对资源进行按需分配。与粗粒度模式一样，应用程序启动时，先会启动executor，但每个executor占用资源仅仅是自己运行所需的资源，不需要考虑将来要运行的任务，之后，mesos会为每个executor动态分配资源，每分配一些，便可以运行一个新任务，单个Task运行完之后可以马上释放对应的资源。每个Task会汇报状态给Mesos slave和Mesos Master，便于更加细粒度管理和容错，这种调度模式类似于MapReduce调度模式，每个Task完全独立，优点是便于资源控制和隔离，但缺点也很明显，短作业运行延迟大。
　　
　　从yarn和mesos的区别可看出，它们各自有优缺点。因此实际使用中，选择哪种框架，要根据本公司的实际需要而定，可考虑现有的大数据生态环境。如我司采用yarn部署spark，原因是，我司早已有较成熟的hadoop的框架，考虑到使用的方便性，采用了yarn模式的部署。