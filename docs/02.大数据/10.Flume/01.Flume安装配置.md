### 一. 安装(flume-ng)
1. 下载安装包 `apache-flume-1.9.0-bin.tar.gz`
2. 上传 解压安装 `tar -zxvf apache-flume-1.9.0-bin.tar.gz`
3. 重命名 `mv apache-flume-1.9.0-bin/ flume`
4. 配置环境变量 `vim ~/.bashrc`
```
# flume环境变量
export FLUME_HOME=/soft/flume
export PATH=$PATH:$FLUME_HOME/bin
```
`source ~/.bashrc`

### 安装 netcat 工具
安装:`sudo yum install -y nc`
判断 44444 端口是否被占用 `sudo netstat -tunlp | grep 44444`

### 二. 修改配置文件
1. flume-env.sh文件配置
- 到${FLUME_HOME}/conf下，复制一份flume-env.sh文件
`cp flume-env.sh.template flume-env.sh`
- 编辑`vim flume-env.sh`
```
export JAVA_HOME=/soft/jdk1.8.0_161
```
- 测试下flume是否能够正常运行
`flume-ng version`
2. Flume服务启动
- 创建文件夹  `mkdir logstodfs`
- 我们在conf文件夹下，创建一个`vim flume_hdfs.conf`
添加内容
```
#source,channel,sink，它们分别都可以配置多份，比如n个channel和n个sink
#先配置单通道，定义source,channel,sink，它们分别都可以配置多份，比如n个channel和n个sink
#agent1 是该agent的名字，在启动的时候需要指定agent的名字
agent1.sources=source1
agent1.channels=channel1
agent1.sinks=sink1

##############配置source###################
#source的类型
agent1.sources.source1.type=spooldir
#spooldir类型的source监控的目录
agent1.sources.source1.spoolDir=/soft/flume/logstodfs
agent1.sources.source1.fileHeader=false
agent1.sources.source1.channels=channel1
agent1.sources.source1.interceptors=i1
agent1.sources.source1.interceptors.i1.type=timestamp
#0.0.0.0表示本机
#agent1.sources.source1.bind=0.0.0.0
#使用的端口
#agent1.sources.source1.port=44445
#指定channel类型
agent1.channels.channel1.type=file
#file channle checkpoint文件的路径
agent1.channels.channel1.checkpointDir=/soft/flume/tmp/point
# file channel data文件的路径
agent1.channels.channel1.dataDirs=/soft/flume/tmp

#指定sink类型
agent1.sinks.sink1.type=hdfs
agent1.sinks.sink1.hdfs.path=hdfs://hadoop1:9000/flume
agent1.sinks.sink1.hdfs.fileType=DataStream
agent1.sinks.sink1.hdfs.writeFormat=TEXT
#多久生成新的文件
agent1.sinks.sink1.hdfs.rollInterval=5
agent1.sinks.sink1.hdfs.rollSize=1000
agent1.sinks.sink1.hdfs.rollCount=0
agent1.sinks.sink1.hdfs.filePrefix=%Y-%m-%d
agent1.sinks.sink1.hdfs.fileSuffix=.txt

agent1.sinks.sink1.channel = channel1
```
- 启动服务 
`flume-ng agent -c /soft/flume/conf -f /soft/flume/conf/flume_hdfs.conf -n agent1 -Dflume.root.logger=INFO,console`
同
```
flume-ng agent -c $FLUME_HOME/conf/
 -f  $FLUME_HOME/conf/flume_hdfs.conf 
-n agent1
-Dflume.root.logger=INFO,console
```
其中flume-ng agent为固定写法
-c指定flume-env.sh文件所在目录
-f指定flume-hdfs.conf文件所在位置
-n指定要启动的agent名称，我们在配置文件中配置的名称为agent1
-Dflume.root.logger代表日志打印到控制台
当然我们也可以使用nohup命令后台挂起程序

### 三. 数据采集
- 打开另一个终端
- 将某个文件（实验阶段建议不要太大）移动或复制到${FLUME_HOME}/logstohdfs目录下，那么flume就会自动读取该文件的数据并上传到hdfs
`cp /soft/datas/short-student-utf8_classNO1.txt /soft/flume/logstodfs/`
- 查看第一个终端窗口(显示信息)
- 最后我们可以去hdfs系统查看我们采集到的数据文件
 

### 四. 数据导入到hbase中
- 把jar包复制替换到/opt/flume/lib目录下面
```
/soft/hive/lib
hbase-protocol-1.1.1.jar
hbase-client-1.1.1.jar
hbase-common-1.1.1.jar 
hbase-server-1.1.1.jar
hbase-hadoop2-compat-1.1.1.jar 
hbase-hadoop-compat-1.1.1.jar
htrace-core-3.1.0-incubating.jar
```
命令: `cp /soft/hive/lib/{hbase-protocol-1.1.1.jar,hbase-client-1.1.1.jar,hbase-common-1.1.1.jar,hbase-server-1.1.1.jar,hbase-hadoop2-compat-1.1.1.jar,hbase-hadoop-compat-1.1.1.jar,htrace-core-3.1.0-incubating.jar} /soft/flume/lib`
- 启动hbase,创建表 `hbase shell` 创建表 `create "flume_hbase","info"`
- 写配置文件 flume_hbase.conf
- 启动服务 `flume-ng agent --conf-file /soft/flume/conf/flume-hbase.conf -n agent2 -Dflume.root.logger=INFO,console`
- 生成数据: 
`echo "hello flume">>/soft/flume/tmp/datas/flume_hbase.txt`
`echo "hello hbase">>/soft/flume/tmp/datas/flume_hbase.txt`

### 五. 数据导入到hive中
1. 写配置文件 flume_hive.conf
2. Flume 端服务器 hosts 配置文件修改(不整可否???)
```
vim /etc/hosts

# myCluster 是你的集群名称，hdfs://myCluster/hellowold 通常用于 HDFS NameNode HA 模式下会用到这个地址
192.168.1.1 node1.hadoop.com myCluster
192.168.1.2 ndoe2.hadoop.com myCluster
192.168.1.3 node3.hadoop.com
```
3. Hive 建表
- 需要开启的策略 - ORC 格式存储 - 分桶 - 支持事务性 - 显式声明 transtions(也可以修改配置文件hive-site.xml)
实测建表的时候不需要分桶。分桶会将文件分散,一个桶编号一个文件。日志收集写到 Hive 中就是为了实现数据聚合，解决小文件问题。
```
-- 是否支持并发
SET hive.support.concurrency = true;
-- 分桶是否被强制执行
SET hive.enforce.bucketing = true;
-- 非严格模式
SET hive.exec.dynamic.partition.mode = nonstrict;
SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
-- 是否开启事务性
SET hive.compactor.initiator.on = true;

-- 工作线程
SET hive.compactor.worker.threads = 1;

-- 建表
CREATE TABLE <MY_HIVE_DB>.<MY_HIVE_TABLE>(
  name string comment '姓名'
  age string comment '年龄'
  role string comment '角色'
)
-- 表明备注 
COMMENT '你好,中国'
-- 创建分区
PARTITIONED BY (
    country STRING comment '国家',
    month STRING comment '月份'
)
-- ORC 支持
STORED AS ORC TBLPROPERTIES ('transactional'='true');

CREATE TABLE flume_hive.flume_hive(name string comment '姓名',age string comment '年龄',role string comment '角色')
COMMENT '你好,中国'
PARTITIONED BY (country STRING comment '国家',month STRING comment '月份')
STORED AS ORC;
```
4. HDFS 目录权限修改
`hadoop fs -chmod 777 -R /usr`(未测试)
5. Flume 写 Hive 依赖包(一说/soft/hive/hcatalog/share/hcatalog/下所有包)
```
calcite-core-1.16.0.3.0.0.0-1634.jar
libfb303-0.9.3.jar
hive-hcatalog-core-3.1.0.3.0.0.0-1634.jar
hadoop-mapreduce-client-core-3.1.0.3.0.0.0-1634.jar
hive-exec-3.1.0.3.0.0.0-1634.jar
hive-standalone-metastore-3.1.0.3.0.0.0-1634.jar
hive-hcatalog-streaming-3.1.0.3.0.0.0-1634.jar
```
6. 启动 Flume 服务
`flume-ng agent -c conf/ -f /soft/flume/conf/flume_hive.conf -n agent3 -Dflume.root.logger=INFO,console`
7. 趁热打铁：记录自己查错的思考方向
- 输入文件格式必须为csv 或json
- 输出格式必须为orc格式
- 看日志文件(呃 暂时没找到)
- 修改hive-site配置文件(永久参数)
- 建表时必须分桶 分区 orc (挖坑记得加上)
- flume配置文件sink的配置(端口号9083 必须有分区(partation)？)  最好参考官方文档
- channel配置(连接 内存) source监控文件夹
- MySQL下hive元数据(metastore)插入值 然后 commit;？
- 启动时hive (hive --service metastore -p 9083) hive --service hiveserver2
- hive命令行设置临时参数