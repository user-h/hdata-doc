(window.webpackJsonp=window.webpackJsonp||[]).push([[40],{369:function(a,s,e){"use strict";e.r(s);var r=e(3),t=Object(r.a)({},(function(){var a=this,s=a._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[s("h1",{attrs:{id:"集群规划"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#集群规划"}},[a._v("#")]),a._v(" 集群规划")]),a._v(" "),s("p",[a._v("首先我们要确认我们的Linux主机是否安装了scala，如果没有安装则需要安装，5台机器都需要安装")]),a._v(" "),s("p",[a._v("学习scala时使用的是2.12版本，所以我们选择spark2.4.2及以上的版本")]),a._v(" "),s("h1",{attrs:{id:"spark-standalone模式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#spark-standalone模式"}},[a._v("#")]),a._v(" Spark Standalone模式")]),a._v(" "),s("h2",{attrs:{id:"_1-安装使用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-安装使用"}},[a._v("#")]),a._v(" 1.安装使用")]),a._v(" "),s("h3",{attrs:{id:"_1-进入spark安装目录下的conf文件夹"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-进入spark安装目录下的conf文件夹"}},[a._v("#")]),a._v(" 1）进入spark安装目录下的conf文件夹")]),a._v(" "),s("p",[s("code",[a._v("cd spark/conf/")])]),a._v(" "),s("h3",{attrs:{id:"_2-修改配置文件名称"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-修改配置文件名称"}},[a._v("#")]),a._v(" 2）修改配置文件名称")]),a._v(" "),s("p",[s("code",[a._v("mv slaves.template slaves")]),a._v(" "),s("code",[a._v("mv spark-env.sh.template spark-env.sh")])]),a._v(" "),s("h3",{attrs:{id:"_3-修改slave文件-添加work节点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-修改slave文件-添加work节点"}},[a._v("#")]),a._v(" 3）修改slave文件，添加work节点：")]),a._v(" "),s("p",[s("code",[a._v("vim slaves")])]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("hadoop101\nhadoop102\nhadoop103\n")])])]),s("h3",{attrs:{id:"_4-修改spark-env-sh文件-添加如下配置"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-修改spark-env-sh文件-添加如下配置"}},[a._v("#")]),a._v(" 4）修改spark-env.sh文件，添加如下配置：")]),a._v(" "),s("p",[s("code",[a._v("vim spark-env.sh")])]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("export JAVA_HOME=/soft/module/jdk1.8.0_161\nexport SPARK_MASTER_HOST=hadoop100\nexport SPARK_MASTER_PORT=7077\n")])])]),s("h3",{attrs:{id:"_5-分发spark包"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-分发spark包"}},[a._v("#")]),a._v(" 5）分发spark包")]),a._v(" "),s("p",[s("code",[a._v("xsync spark/")])]),a._v(" "),s("h3",{attrs:{id:"_6-启动-在master"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-启动-在master"}},[a._v("#")]),a._v(" 6）启动(在Master)")]),a._v(" "),s("p",[s("code",[a._v("sbin/start-all.sh")]),a._v(" "),s("code",[a._v("xcall.sh")])]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("------------------- hadoop100 --------------\n10021 Jps\n9944 Master\n------------------- hadoop101 --------------\n9159 Jps\n9096 Worker\n------------------- hadoop102 --------------\n8740 Worker\n8804 Jps\n------------------- hadoop103 --------------\n8749 Worker\n8813 Jps\n")])])]),s("p",[a._v("网页查看：hadoop100:8080\n注意：如果遇到 “JAVA_HOME not set” 异常，可以在sbin目录下的spark-config.sh 文件中加入如下配置：\nexport JAVA_HOME=XXXX")]),a._v(" "),s("h3",{attrs:{id:"_7-官方求pi案例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_7-官方求pi案例"}},[a._v("#")]),a._v(" 7）官方求PI案例")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("bin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--executor-memory 1G \\\n--total-executor-cores 2 \\\n./examples/jars/spark-examples_2.12-3.0.0-preview2.jar \\\n100\n")])])]),s("h3",{attrs:{id:"_8-启动spark-shell"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_8-启动spark-shell"}},[a._v("#")]),a._v(" 8）启动spark shell")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("/soft/module/spark/bin/spark-shell \\\n--master spark://hadoop100:7077 \\\n--executor-memory 1g \\\n--total-executor-cores 2\n")])])]),s("p",[a._v("参数：--master spark://hadoop100:7077指定要连接的集群的master\n执行WordCount程序")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v('scala>sc.textFile("input").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect\n\nres0: Array[(String, Int)] = Array((hadoop,6), (oozie,3), (spark,3), (hive,3), (atguigu,3), (hbase,6))\n\nscala>\n')])])]),s("h2",{attrs:{id:"_2-jobhistoryserver配置"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-jobhistoryserver配置"}},[a._v("#")]),a._v(" 2.JobHistoryServer配置")]),a._v(" "),s("h3",{attrs:{id:"_1-修改spark-default-conf-template名称"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-修改spark-default-conf-template名称"}},[a._v("#")]),a._v(" 1）修改spark-default.conf.template名称")]),a._v(" "),s("p",[s("code",[a._v("mv spark-defaults.conf.template spark-defaults.conf")])]),a._v(" "),s("h3",{attrs:{id:"_2-修改spark-default-conf文件-开启log"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-修改spark-default-conf文件-开启log"}},[a._v("#")]),a._v(" 2）修改spark-default.conf文件，开启Log：")]),a._v(" "),s("p",[s("code",[a._v("vim spark-defaults.conf")])]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("spark.eventLog.enabled           true\nspark.eventLog.dir               hdfs://hadoop100:9000/directory\n")])])]),s("p",[a._v("注意：HDFS上的目录需要提前存在。\n"),s("code",[a._v("hadoop fs -mkdir /directory")])]),a._v(" "),s("h3",{attrs:{id:"_3-修改spark-env-sh文件-添加如下配置"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-修改spark-env-sh文件-添加如下配置"}},[a._v("#")]),a._v(" 3）修改spark-env.sh文件，添加如下配置：")]),a._v(" "),s("p",[s("code",[a._v("vim spark-env.sh")])]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v('export SPARK_HISTORY_OPTS="\n-Dspark.history.ui.port=18080 \n-Dspark.history.retainedApplications=30 \n-Dspark.history.fs.logDirectory=hdfs://hadoop100:9000/directory"\n')])])]),s("p",[a._v("参数描述：\nspark.eventLog.dir：Application在运行过程中所有的信息均记录在该属性指定的路径下；")]),a._v(" "),s("p",[a._v("spark.history.ui.port=18080  WEBUI访问的端口号为18080")]),a._v(" "),s("p",[a._v("spark.history.fs.logDirectory=hdfs://hadoop102:9000/directory  配置了该属性后，在start-history-server.sh时就无需再显式的指定路径，Spark History Server页面只展示该指定路径下的信息")]),a._v(" "),s("p",[a._v("spark.history.retainedApplications=30指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。")]),a._v(" "),s("h3",{attrs:{id:"_4-分发配置文件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-分发配置文件"}},[a._v("#")]),a._v(" 4）分发配置文件")]),a._v(" "),s("p",[s("code",[a._v("xsync spark-defaults.conf")]),a._v(" "),s("code",[a._v("xsync spark-env.sh")])]),a._v(" "),s("h3",{attrs:{id:"_5-启动历史服务"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-启动历史服务"}},[a._v("#")]),a._v(" 5）启动历史服务")]),a._v(" "),s("p",[s("code",[a._v("sbin/stop-history-server.sh")])]),a._v(" "),s("h3",{attrs:{id:"_6-再次执行任务"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-再次执行任务"}},[a._v("#")]),a._v(" 6）再次执行任务")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("bin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--executor-memory 1G \\\n--total-executor-cores 2 \\\n./examples/jars/spark-examples_2.12-3.0.0-preview2.jar \\\n100\n")])])]),s("h3",{attrs:{id:"_7-查看历史服务"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_7-查看历史服务"}},[a._v("#")]),a._v(" 7）查看历史服务")]),a._v(" "),s("p",[a._v("hadoop100:18080")]),a._v(" "),s("h2",{attrs:{id:"_3-ha配置"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-ha配置"}},[a._v("#")]),a._v(" 3.HA配置")]),a._v(" "),s("h3",{attrs:{id:"_1-zookeeper正常安装并启动"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-zookeeper正常安装并启动"}},[a._v("#")]),a._v(" 1）zookeeper正常安装并启动")]),a._v(" "),s("h3",{attrs:{id:"_2-修改spark-env-sh文件添加如下配置"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-修改spark-env-sh文件添加如下配置"}},[a._v("#")]),a._v(" 2）修改spark-env.sh文件添加如下配置：")]),a._v(" "),s("p",[s("code",[a._v("vim spark-env.sh")])]),a._v(" "),s("p",[a._v("注释掉如下内容：")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("#SPARK_MASTER_HOST=hadoop100\n#SPARK_MASTER_PORT=7077\n")])])]),s("p",[a._v("添加上如下内容：")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v('export SPARK_DAEMON_JAVA_OPTS="\n-Dspark.deploy.recoveryMode=ZOOKEEPER \n-Dspark.deploy.zookeeper.url=hadoop101,hadoop102,hadoop103 \n-Dspark.deploy.zookeeper.dir=/spark"\n')])])]),s("h3",{attrs:{id:"_3-分发配置文件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-分发配置文件"}},[a._v("#")]),a._v(" 3）分发配置文件")]),a._v(" "),s("p",[s("code",[a._v("xsync spark-env.sh")])]),a._v(" "),s("h3",{attrs:{id:"_4-在hadoop100上-master-启动全部节点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-在hadoop100上-master-启动全部节点"}},[a._v("#")]),a._v(" 4）在hadoop100上(Master)启动全部节点")]),a._v(" "),s("p",[s("code",[a._v("sbin/start-all.sh")])]),a._v(" "),s("h3",{attrs:{id:"_5-在hadoop101上单独启动master节点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-在hadoop101上单独启动master节点"}},[a._v("#")]),a._v(" 5）在hadoop101上单独启动master节点")]),a._v(" "),s("p",[s("code",[a._v("sbin/start-master.sh")])]),a._v(" "),s("h3",{attrs:{id:"_6-spark-ha集群访问"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-spark-ha集群访问"}},[a._v("#")]),a._v(" 6）spark HA集群访问")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("/soft/module/spark/bin/spark-shell \\\n--master spark://hadoop100:7077,hadoop102:7077 \\\n--executor-memory 2g \\\n--total-executor-cores 2\n")])])]),s("h3",{attrs:{id:"_7-执行程序"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_7-执行程序"}},[a._v("#")]),a._v(" 7) 执行程序")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("bin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://hadoop100:7077,hadoop101:7077 \\\n--executor-memory 1G \\\n--total-executor-cores 2 \\\n./examples/jars/spark-examples_2.12-3.0.0-preview2.jar \\\n100\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v('./spark-shell --master spark://hadoop100:7077,hadoop101:7077\n\nhadoop fs -mkdir -p /spark/input\nhadoop fs -put RELEASE /spark/input\n\nsc.textFile("/spark/input").flatMap(_.split(" ")).map(word=>(word,1)).reduceByKey(_+_).map(entry=>(entry._2,entry._1)).sortByKey(false,1).map(entry=>(entry._2,entry._1)).saveAsTextFile("/spark/output/")\n')])])]),s("h3",{attrs:{id:"附-修改sparkui界面默认端口号"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#附-修改sparkui界面默认端口号"}},[a._v("#")]),a._v(" 附:修改SparkUI界面默认端口号")]),a._v(" "),s("p",[a._v("SparkUI界面默认端口号为8080(可能会被占用,被占用后默认+1HTTP ERROR 404 Not Found),两种方法修改")]),a._v(" "),s("ol",[s("li",[a._v("修改conf/spark-env.sh, 加上"),s("code",[a._v("export SPARK_MASTER_WEBUI_PORT=8082")])]),a._v(" "),s("li",[a._v("修改 sbin/start-master.sh")])]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v('if [ "$SPARK_MASTER_WEBUI_PORT" = "" ]; then\n  SPARK_MASTER_WEBUI_PORT=8082\nfi\n')])])]),s("h1",{attrs:{id:"spark-yarn模式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#spark-yarn模式"}},[a._v("#")]),a._v(" Spark Yarn模式")]),a._v(" "),s("h2",{attrs:{id:"_1-安装使用-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-安装使用-2"}},[a._v("#")]),a._v(" 1.安装使用")]),a._v(" "),s("h3",{attrs:{id:"_1-修改hadoop配置文件yarn-site-xml"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-修改hadoop配置文件yarn-site-xml"}},[a._v("#")]),a._v(" 1）修改hadoop配置文件yarn-site.xml")]),a._v(" "),s("p",[a._v("添加如下内容："),s("code",[a._v("vim yarn-site.xml")])]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("        \x3c!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --\x3e\n        <property>\n                <name>yarn.nodemanager.pmem-check-enabled</name>\n                <value>false</value>\n        </property>\n        \x3c!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --\x3e\n        <property>\n                <name>yarn.nodemanager.vmem-check-enabled</name>\n                <value>false</value>\n        </property>\n")])])]),s("h3",{attrs:{id:"_2-修改spark-env-sh"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-修改spark-env-sh"}},[a._v("#")]),a._v(" 2）修改spark-env.sh")]),a._v(" "),s("p",[a._v("添加如下配置："),s("code",[a._v("vim spark-env.sh")])]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("YARN_CONF_DIR=/soft/module/hadoop-2.9.2/etc/hadoop\n")])])]),s("h3",{attrs:{id:"_3-分发配置文件-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-分发配置文件-2"}},[a._v("#")]),a._v(" 3）分发配置文件")]),a._v(" "),s("p",[s("code",[a._v("xsync /soft/module/hadoop-2.9.2/etc/hadoop/yarn-site.xml")]),a._v(" "),s("code",[a._v("xsync spark-env.sh")])]),a._v(" "),s("h3",{attrs:{id:"_4-执行一个程序"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-执行一个程序"}},[a._v("#")]),a._v(" 4）执行一个程序")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("bin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode client \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n100\n")])])]),s("p",[a._v("注意：在提交任务之前需启动HDFS以及YARN集群。")]),a._v(" "),s("h2",{attrs:{id:"_2-日志查看"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-日志查看"}},[a._v("#")]),a._v(" 2. 日志查看")]),a._v(" "),s("h3",{attrs:{id:"_1-修改配置文件spark-defaults-conf"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-修改配置文件spark-defaults-conf"}},[a._v("#")]),a._v(" 1）修改配置文件spark-defaults.conf")]),a._v(" "),s("p",[a._v("添加如下内容：")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("spark.yarn.historyServer.address=hadoop102:18080\nspark.history.ui.port=18080\n")])])]),s("h3",{attrs:{id:"_2-重启spark历史服务"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-重启spark历史服务"}},[a._v("#")]),a._v(" 2）重启spark历史服务")]),a._v(" "),s("p",[s("code",[a._v("sbin/stop-history-server.sh")])]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("stopping org.apache.spark.deploy.history.HistoryServer\n")])])]),s("p",[s("code",[a._v("sbin/start-history-server.sh")])]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("starting org.apache.spark.deploy.history.HistoryServer, logging to /opt/module/spark/logs/spark-atguigu-org.apache.spark.deploy.history.HistoryServer-1-hadoop102.out\n")])])]),s("h3",{attrs:{id:"_3-提交任务到yarn执行"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-提交任务到yarn执行"}},[a._v("#")]),a._v(" 3）提交任务到Yarn执行")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[a._v("bin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode client \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n100\n")])])]),s("h3",{attrs:{id:"_4-web页面查看日志"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-web页面查看日志"}},[a._v("#")]),a._v(" 4）Web页面查看日志")]),a._v(" "),s("p",[s("code",[a._v("hadoop100:8088")]),a._v("\n``")]),a._v(" "),s("h1",{attrs:{id:"附-spark-三种部署模式的区别对比"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#附-spark-三种部署模式的区别对比"}},[a._v("#")]),a._v(" 附：spark 三种部署模式的区别对比")]),a._v(" "),s("p",[a._v("源网页:https://www.cnblogs.com/eric666666/p/11228825.html")]),a._v(" "),s("p",[a._v("在这三种部署模式中，standalone作为spark自带的分布式部署模式，是最简单也是最基本的spark应用程序部署模式，这里就不再赘述。")]),a._v(" "),s("p",[a._v("这里就讲一下yarn和mesos的区别：")]),a._v(" "),s("p",[a._v("(1) 就两种框架本身而言，mesos上可部署yarn框架。而yarn是更通用的一种部署框架，而且技术较成熟。")]),a._v(" "),s("p",[a._v("(2) mesos双层调度机制，能支持多种调度模式，而Yarn通过Resource　Mananger管理集群资源，只能使用一种调度模式。Mesos 的双层调度机制为：mesos可接入如yarn一般的分布式部署框架，但Mesos要求可接入的框架必须有一个调度器模块，该调度器负责框架内部的任务调度。当一个framework想要接入mesos时，需要修改自己的调度器，以便向mesos注册，并获取mesos分配给自己的资源， 这样再由自己的调度器将这些资源分配给框架中的任务，也就是说，整个mesos系统采用了双层调度框架：第一层，由mesos将资源分配给框架；第二层，框架自己的调度器将资源分配给自己内部的任务。")]),a._v(" "),s("p",[a._v("(3) mesos可实现粗、细粒度资源调度，可动态分配资源，而Yarn只能实现静态资源分配。其中粗粒度和细粒度调度定义如下：")]),a._v(" "),s("p",[a._v("粗粒度模式（Coarse-grained Mode）：程序运行之前就要把所需要的各种资源（每个executor占用多少资源，内部可运行多少个executor）申请好，运行过程中不能改变。\n　　\n　　细粒度模式（Fine-grained Mode）：为了防止资源浪费，对资源进行按需分配。与粗粒度模式一样，应用程序启动时，先会启动executor，但每个executor占用资源仅仅是自己运行所需的资源，不需要考虑将来要运行的任务，之后，mesos会为每个executor动态分配资源，每分配一些，便可以运行一个新任务，单个Task运行完之后可以马上释放对应的资源。每个Task会汇报状态给Mesos slave和Mesos Master，便于更加细粒度管理和容错，这种调度模式类似于MapReduce调度模式，每个Task完全独立，优点是便于资源控制和隔离，但缺点也很明显，短作业运行延迟大。\n　　\n　　从yarn和mesos的区别可看出，它们各自有优缺点。因此实际使用中，选择哪种框架，要根据本公司的实际需要而定，可考虑现有的大数据生态环境。如我司采用yarn部署spark，原因是，我司早已有较成熟的hadoop的框架，考虑到使用的方便性，采用了yarn模式的部署。")])])}),[],!1,null,null,null);s.default=t.exports}}]);