(window.webpackJsonp=window.webpackJsonp||[]).push([[42],{371:function(a,t,e){"use strict";e.r(t);var n=e(3),r=Object(n.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h2",{attrs:{id:"集群规划"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#集群规划"}},[a._v("#")]),a._v(" 集群规划")]),a._v(" "),t("table",[t("thead",[t("tr",[t("th",[a._v("主机")]),a._v(" "),t("th",[a._v("Zookeeper")]),a._v(" "),t("th",[a._v("jobmanager")]),a._v(" "),t("th",[a._v("taskmanager")])])]),a._v(" "),t("tbody",[t("tr",[t("td",[a._v("hadoop100")]),a._v(" "),t("td"),a._v(" "),t("td",[a._v("是")]),a._v(" "),t("td")]),a._v(" "),t("tr",[t("td",[a._v("hadoop101")]),a._v(" "),t("td",[a._v("是")]),a._v(" "),t("td",[a._v("是")]),a._v(" "),t("td",[a._v("是")])]),a._v(" "),t("tr",[t("td",[a._v("hadoop102")]),a._v(" "),t("td",[a._v("是")]),a._v(" "),t("td"),a._v(" "),t("td",[a._v("是")])]),a._v(" "),t("tr",[t("td",[a._v("hadoop103")]),a._v(" "),t("td",[a._v("是")]),a._v(" "),t("td"),a._v(" "),t("td",[a._v("是")])])])]),a._v(" "),t("h2",{attrs:{id:"一-standalone-cluster-ha"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#一-standalone-cluster-ha"}},[a._v("#")]),a._v(" 一. standalone cluster HA")]),a._v(" "),t("h3",{attrs:{id:"_1-安装"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-安装"}},[a._v("#")]),a._v(" 1. 安装")]),a._v(" "),t("ol",[t("li",[a._v("下载安装包 "),t("code",[a._v("flink-1.9.2-bin-scala_2.12.tgz")])]),a._v(" "),t("li",[a._v("上传解压\n"),t("code",[a._v("tar -zxvf flink-1.9.2-bin-scala_2.12.tgz -C /soft/module/")]),a._v(" 重命名 "),t("code",[a._v("mv flink-1.9.2/ flink")])]),a._v(" "),t("li",[a._v("配置环境变量\n"),t("code",[a._v("sudo vim /etc/profile")])])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# flink环境变量\nexport FLINK_HOME=/soft/module/flink\nexport PATH=$PATH:$FLINK_HOME/bin\n")])])]),t("p",[t("code",[a._v("source /etc/profile")])]),a._v(" "),t("h3",{attrs:{id:"_2-修改配置文件"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-修改配置文件"}},[a._v("#")]),a._v(" 2.修改配置文件")]),a._v(" "),t("ol",[t("li",[a._v("修改zoo.cfg文件\n"),t("code",[a._v("vim conf/zoo.cfg")]),a._v(" 和Zookeeper配置相同")])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# The port at which the clients will connect\nclientPort=2181\n#######################cluster##########################\nserver.1=hadoop101:2888:3888\nserver.2=hadoop102:2888:3888\nserver.3=hadoop103:2888:3888\n")])])]),t("ol",{attrs:{start:"2"}},[t("li",[a._v("修改flink-conf.yaml文件(需要注意的是 配置:后要有空格)\n"),t("code",[a._v("vim conf/flink-conf.yaml")])])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# jobmanager服务主机名,这条配置只在单机模式下有用 用于taskmanager寻找jobmanager 及后续通信\n# 集群模式下这条会被忽略,集群模式下flink会寻找conf/master中的主机配置\njobmanager.rpc.address: hadoop100\n#jobmanager rpc 端口号\njobmanager.rpc.port: 6123\n#jobmanager内存\njobmanager.heap.size: 1024m\n#jobmanager内存\ntaskmanager.heap.size: 1024m\n# taskmanager服务中插槽个数,其实就是子任务task的个数\ntaskmanager.numberOfTaskSlots: 1\n# The parallelism used for programs that did not specify and other parallelism.\n# 程序默认并行度\nparallelism.default: 1\n\n# 开启集群模式\nhigh-availability: zookeeper\n# 开启hdfs存储目录\nhigh-availability.storageDir: hdfs:///flink/ha/\n# 指定Zookeeper集群地址\nhigh-availability.zookeeper.quorum: hadoop101:2181,hadoop102:2181,hadoop103:2181\n\n# <class-name-of-factory>.\n# 启用检查点,存储于文件系统\nstate.backend: filesystem\n\n# Directory for checkpoints filesystem, when using any of the default bundled\n# state backends.\n# 检查点的文件存储位置,这里我们指定的事hdfs集群地址\n# 这里的检查点是指程序运行过程中的检查点,以便快速恢复程序运行或恢复到某个正确运行的时刻\nstate.checkpoints.dir: hdfs://mycluster:9000/flink-checkpoints\n# Default target directory for savepoints, optional.\n# 对应外部应用程序的检查点文件存储位置  记录数据的处理进度\n# 例如flink处理到30% 重启了 启动之后在30%的位置继续处理\nstate.savepoints.dir: hdfs://mycluster:9000/flink-checkpoints\n\n# full模式是一个任务失败了,结束所有任务,然后重启所有任务  简单粗暴但是龙一直造成内存溢出\n# region模式是查出具体出错的任务,然后单独重启这一任务\njobmanager.execution.failover-strategy: region\n")])])]),t("ol",{attrs:{start:"3"}},[t("li",[a._v("修改masters和slaves文件\n"),t("code",[a._v("vim conf/masters")])])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("hadoop100:8081\nhadoop101:8081\n")])])]),t("p",[t("code",[a._v("vim conf/slaves")])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("hadoop101\nhadoop102\nhadoop103\n")])])]),t("ol",{attrs:{start:"4"}},[t("li",[a._v("最后，当前的flink版本是一个纯净的版本，如果需要依赖其他系统（例如咱们当前flink的相关数据都存储到了hdfs），则需要添加相应的jar包\n官网有提供：https://flink.apache.org/downloads.html#additional-components\n"),t("code",[a._v("mv /soft/software/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar /soft/module/flink/lib/")])]),a._v(" "),t("li",[a._v("分发文件并修改环境变量")])]),a._v(" "),t("h3",{attrs:{id:"_3-启动服务"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-启动服务"}},[a._v("#")]),a._v(" 3. 启动服务")]),a._v(" "),t("p",[a._v("启动之前我们要确保我们的zookeeper集群已启动，并且hdfs集群是启动状态")]),a._v(" "),t("ul",[t("li",[a._v("直接使用"),t("code",[a._v("start-cluster.sh")]),a._v("脚本启动")]),a._v(" "),t("li",[a._v("jps查看进程")]),a._v(" "),t("li",[a._v("访问flink的web UI hadoop100:8081")])]),a._v(" "),t("hr"),a._v(" "),t("p",[a._v("当前flink虽然使用了hdfs系统，但是其运行模式还是Stand alone，就是独立集群\n独立集群在执行任务时所有的资源分配管理都是flink自己安排的\n我们尝试使用独立集群运行一个测试计算\nFlink可以读取系统文件，也可以读取hdfs文件\n我们以本地系统文件为例\n我们使用现有的WordCount.jar去统计README.txt文件的内容并输出\n这个输出结果被输出到taskmanager机器，是随机的\n所以我们最好还是使用hdfs系统")]),a._v(" "),t("ul",[t("li",[a._v("测试\n"),t("code",[a._v("flink run ./examples/batch/WordCount.jar --input hdfs://mycluster/datas/wordcounttest.txt --output hdfs://mycluster/datas/wordcountresult.txt")])])]),a._v(" "),t("h2",{attrs:{id:"二-flink-on-yarn-yarn-cluster-ha"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#二-flink-on-yarn-yarn-cluster-ha"}},[a._v("#")]),a._v(" 二. flink on yarn(Yarn cluster HA )")]),a._v(" "),t("p",[a._v("在上面的基础上 参考链接: https://www.jianshu.com/p/8f1e650ebcad")]),a._v(" "),t("h3",{attrs:{id:"_0-yarn高可用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_0-yarn高可用"}},[a._v("#")]),a._v(" 0. yarn高可用？")]),a._v(" "),t("h3",{attrs:{id:"_1-修改配置文件"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-修改配置文件"}},[a._v("#")]),a._v(" 1. 修改配置文件")]),a._v(" "),t("ol",[t("li",[a._v("配置yarn-site.xml")])]),a._v(" "),t("ul",[t("li",[a._v("修改环境变量 "),t("code",[a._v("vim /etc/profile")]),a._v("\n加上(用于flink寻找yarn的配置信息\n): "),t("code",[a._v("export HADOOP_CONF_DIR=/soft/module/hadoop-2.9.2/etc/hadoop/")]),a._v(" "),t("code",[a._v("source /etc/profile")])]),a._v(" "),t("li",[a._v("修改文件: "),t("code",[a._v("vim yarn-site.xml")])])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("<property>\n    \x3c!--配置yarn最大重试次数--\x3e\n  <name>yarn.resourcemanager.am.max-attempts</name>\n  <value>4</value>\n</property>\n")])])]),t("ol",{attrs:{start:"2"}},[t("li",[a._v("配置flink-conf.yaml")])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# 配置Yarn重试次数\nyarn.application-attempts: 10\n\n## 配置Zookeeper\n high-availability: zookeeper\n high-availability.storageDir: hdfs:///flink/ha/\n high-availability.zookeeper.quorum: 10.108.4.203:2181,10.108.4.204:2181,10.108.4.205:2181\n high-availability.zookeeper.path.root: /flink\n high-availability.cluster-id: /cluster_yarn\n")])])]),t("ol",{attrs:{start:"3"}},[t("li",[a._v("同步配置文件")])]),a._v(" "),t("h3",{attrs:{id:"_2-启动"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-启动"}},[a._v("#")]),a._v(" 2. 启动")]),a._v(" "),t("ol",[t("li",[a._v("启动Flink Yarn Session有2种模式：分离模式、客户端模式")]),a._v(" "),t("li",[a._v("分离模式\n通过-d指定分离模式，即客户端在启动Flink Yarn Session后，就不再属于Yarn Cluster的一部分。如果想要停止Flink Yarn Application，需要通过yarn application -kill 命令来停止\n"),t("code",[a._v("yarn-session.sh -n 3 -jm 1024 -tm 1024 -s 3 -nm FlinkOnYarnSession -d -st")])])]),a._v(" "),t("h3",{attrs:{id:"错误"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#错误"}},[a._v("#")]),a._v(" 错误")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("2020-04-15 16:19:28,433 ERROR org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Error while running the Flink Yarn session.\norg.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn session cluster\n        at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploySessionCluster(AbstractYarnClusterDescriptor.java:387)\n        at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:616)\n        at org.apache.flink.yarn.cli.FlinkYarnSessionCli.lambda$main$3(FlinkYarnSessionCli.java:844)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)\n        at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)\n        at org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:844)\nCaused by: org.apache.flink.yarn.AbstractYarnClusterDescriptor$YarnDeploymentException: The YARN application unexpectedly switched to state FAILED during deployment. \n")])])]),t("p",[a._v("在yarn-site.xml加上")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("    <property> \n        <name>yarn.nodemanager.vmem-check-enabled</name> \n        <value>false</value> \n    </property> \n")])])])])}),[],!1,null,null,null);t.default=r.exports}}]);